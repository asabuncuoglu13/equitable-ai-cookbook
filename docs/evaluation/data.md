# Data Evaluation Strategies

Throughout the machine learning development pipeline, we evaluate data heavily in the data collection and pre-processing stages. The data collection journey itself unfolds through multiple stages, encompassing Data Gathering, Consolidation, Annotation (particularly relevant in supervised training scenarios), and striving for a harmonious Inter-annotator Agreement. Existing LLMs leverage a diverse array of resources procured from the vast expanse of the internet. These models do not specifically require human supervision in the training but use human-in-the-loop approaches to improve the reliability and cohesion of their outputs. Throughout this data collection process, the unbridled collection of data introduces a host of challenges, ranging from inherent biases and copyright concerns to the inadvertent inclusion of potentially harmful content.

To illustrate what the collected data looks like, here are some examples of data collection methods:

- Most text datasets are generated by web scraping which involves crawling websites to extract text data.
- Or, it can be requested by APIs (Application Programming Interfaces) to collect data from platforms like social media, news websites, and other online sources.
- Researchers produce ready-to-use text corpora which includes more guided and structured data such as books, articles, and research papers.
- Collecting conversational data such as chat logs can help language models to understand and generate human-like responses.
- Code Repositories and programming-related content such as GitHub and StackOverflow can improve the model's understanding of code.
- There are also fine-tuning datasets specific to certain tasks or industries for fine-tuning the model on specific use cases.

We can extend this list further... But, our focus in this chapter is data evaluation techniques. 

After collecting the data, the pre-processing methods come to the fore, playing a pivotal role in refining the gathered data. These methods undertake the tasks of cleansing, structuring, and augmenting the data, all with the overarching goal of enhancing the efficacy of the subsequent training phases. As we delve deeper into the intricate landscape of language model development, it becomes evident that the integrity of the data collection and pre-processing phases lays the foundation for the model's overall performance and, crucially, its ability to navigate and respond to the complexities of human language. In this context, evaluating the possible issues and mitigating them in these preliminary stages becomes imperative for producing a reliable LLM model.

We can categorise the evaluation approaches into three: (1) Supervised (manual) evaluation, (2) Semi-supervised (semi-automated) evaluation, and (3) Unsupervised (automated) evaluation.

## Supervised (manual) evaluation


### Self Evaluation

- **Creating data cards:** Creating data cards in a standardised format (example) can help to understand possible issues beforehand. For example, [The Data Cards Playbook](https://sites.research.google/datacardsplaybook/) asks for data source distribution related to bias (e.g. geographic, and gender), and sampling tasks.
- **Using surveys and frameworks:** 
  1. For example, [UK Statistics Authority's Guidelines](https://uksa.statisticsauthority.gov.uk/publication/guidelines-on-using-the-ethics-self-assessment-process/pages/1/) can help researchers evaluate the data collection practices and evaluation approaches following a structured ethical assessment. 
  2. Pipino et al.’s data quality assessment provides a generalised way to assess data quality It is a well-known assessment approach applied by several businesses in the last decade.
Pipino et al.’s data quality assessment framework offers a standardised and comprehensive approach to evaluating data quality, making it applicable to various industries {cite}`pipino_data_2002`. It has been adopted by businesses in the last two decades to ensure the reliability and usability of their data. By utilising this framework, we can systematically evaluate the data quality and identify areas for improvement.
  3. FAIR principles are the most popular assessment criteria among the reproducible research community {cite}`wilkinson_fair_2016`. The principles _"put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals."_

### Expert Evaluation

One of the most reliable, but expensive solution is annotating bias in the dataset by expert human annotators. We can use the guidelines and frameworks that we mentioned in the "self-evaluation." 

Spinde et al. created a [34 item questionnaire](https://media-bias-research.org/questionnaire-tree/#Head) to accurately map bias perception: <https://media-bias-research.org/publications/> BABE (Bias Annotations By Experts) dataset is created using this approach {cite}`spinde_neural_2021`. 

However, asking for the perception of human annotators is also quite challenging, as the bias can be rather subjective. 


## Unsupervised (automated) evaluation

### Training a classifier to detect issues

One example of this approach is [Dbias bias detection package](https://github.com/dreji18/Fairness-in-AI) uses a language model to detect issues in the text considering ten dimensions (gender, race, social status, age, disability, religion, profession, nationality, education, and body size) {cite}`raza_dbias_2024`. They adopted the Inside-Outside-Beginning (IOB) annotation scheme {cite}`ramshaw_text_1995` to classify and annotate ‘BIAS’ entities. Then, they trained a BERT model to predict BIAS entities in a dataset.

### Comparing transformation in the same vector space


## Semi-supervised (semi-automated) evaluation

### Annotating with active learning

### Comparing the longitudinal representations

Based on the defined threshold, if data augmentation techniques result in different fairness scores, we should consider re-evaluating the dataset or choosing the fairer data transformation methodology.



# Trusted AI Cookbook

Trusted AI layers: Safe, responsible, equitable.

_Safe_ aspect is the core of the trusted AI. It includes sub-components such as robustness, security, and privacy. These characteristics build a highly reliable system for every user.

_Responsible_ aspect is the differentiator of a product towards being reliable to trustable. Developing the system to be fair, transparent, and sustainable are some example characteristics. 

_Equitable_ aspect involves creating and using artificial intelligence systems in a just manner, ensuring they both behave fair, and improves equality and inclusivity in society. We tend to believe technology is neutral and how you use it defines it is a good or bad application of a specific technology. It might be true for some traditional rule-based system (although I also doubt that), but AI by its nature can be ill-designed, since the current systems mostly trained by utilising vast data. Hence, the data collection and selection practices defines its features. Furthermore, access to large computational power is a must in the development process, which limits developing capability to a selected community.

```{note}
This cookbook is a work-in-progress. You can contribute to the development by opening new issues and pull requests on Github.
```

In this cookbook, *trusted AI* refers to a broad term to cover responsible[^rai], trustworthy[^airmf] and inclusive AI[^iai] development.

These terms also requires establishing other characteristics such as security, privacy, and fairness. For example, NIST AI RMF[^airmf] defines *trustworthiness* with seven sub-components: Safe, secure and resilient, explainable and interpretable, privacy-enhanced, fair with harmful bias managed, valid and reliable, and accountable and transparent. Each subcomponent requires a careful evaluation based on specific use cases. In this sense, *trusted AI* is a combination of various established research areas.

![NIST Trustworthy AI Characteristics](./media/nist-trustworthy.png)

This cookbook is an effort to bring *best practices* from the literature and share some hands-on practical experiments to take some confident steps for achieving trustworthiness in the era of *large foundational models*. "Best practices" can widely refer to a set of techniques, methodologies, processes, or guidelines that are accepted as superior or most effective in a particular field or context. When the desired outcome is achieving equal access/outcomes/participation/benefits and *trustworthiness*, defining best practices becomes challenging, as these are complex sociotechnical terms.

This repository contains an extensive list of techniques and experiments as an initial effort to achieve this goal in the "digital economy" domain. You are welcome to add your use cases via pull reqest. In the digital economy domain, we selected three use cases:

::::{grid}
:gutter: 2

:::{grid-item-card}
**Fair Use of LLMs in Finance**

Developing more fair and equitable applications (e.g. credit scoring, financial news sentiment analysis, market prediction) powered by AI.

[➡️ See the content](./usecases/finance/introduction.md)
:::

:::{grid-item-card}
**A Fairness Benchmark for Facial Biometrics**

Improving biometric facial verification using synthetic data generated by foundational models (e.g. diffusion models)

[➡️ See the content](./usecases/biometrics/introduction.md)

:::
::::

In a complex product development ecosystem, it is hard to achive "trustworthiness," as all sub-modules of a product is expected to achieve trustworthiness. Therefore, the desirable way to achieve trustworthiness is following a proactive "by design" approach in the product design. In this cookbook, we explored the ways to achieve this kind of proactive approach, however implementing them in a product lifecycle is still up to technical capacity of development teams.

## Related Materials

This cookbook assumes, you already have some understanding on AI and maybe developed an AI-enabled product, but limited experience in assessing, managing and monitoring safety, equity and fairness of these products. Below, we listed some wonderful open-source books that can help you adopt best-practices of ML development in a broader context:

- [The Turing Way:](https://book.the-turing-way.org/index.html) A guide for reproducible project design, development and management.
- [Techwork's Best Practice Guide:](https://techworkshub.github.io/best-practice-guide/index.html) Five essential questions: (1) Should I use AI/ML? (2) How do I define my AI/ML project (3) How do I collect data? (4) How do I train my AI/ML application? (5) How do I deploy my AI/ML application?

## Contents

Although implementing "best practices" in trusted AI domain requires a cross disciplinary effort, we focused on the sociotechnical and technical techniques, particularly fairness, security and privacy. We do not go into details on governance, auditing techniques, comprehensive assurance.

```{tableofcontents}
```

[^airmf]: The trustworthy AI definition is based on NIST's AI RMF: https://www.nist.gov/itl/ai-risk-management-framework
[^rai]: RAI definition is based on a combination of industry best practices, including [Google](https://research.google/teams/responsible-ai/), (Microsoft)(https://www.microsoft.com/en-us/ai/responsible-ai), [Accenture](https://www.accenture.com/gb-en/case-studies/data-ai/blueprint-responsible-ai)
[^iai]: The inclusiveness is based on inclusive HCI principles of [Microsoft](https://inclusive.microsoft.design/)
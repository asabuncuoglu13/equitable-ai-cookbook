# Equitable AI Cookbook

Equitable AI involves creating and using artificial intelligence systems in a fair and just manner, ensuring they do not reinforce existing biases or discriminate against certain groups. It focuses on fairness, transparency, accountability, inclusivity, and ethical considerations to prevent unfair outcomes and promote equal opportunities for all individuals and communities.

In this cookbook, I use the term "equitable AI" as a broad term includes responsible, trustworthy and inclusive AI development. In this sense, it is a combination of various already-established research areas. This cookbook is an effort to bring available resources from existing research to guide us taking some confident steps to achieve trustworthiness in the era of *large foundational models*. It is an effort to bring the "best practices" together to achieve equitable AI, particularly focusin on the use cases from public services and digital economy. "Best practices" can widely refer to a set of techniques, methodologies, processes, or guidelines that are accepted as superior or most effective in a particular field or context. When the desired outcome is achieving equal access/outcomes/participation/benefits and trustworthiness, defining the "best practices" becomes challenging as these are complex sociotechnical terms. For example, NIST defines "trustworthiness" with seven sub-components, and each subcomponent requires a careful evaluation based on specific use cases. 

There have already been many good papers and books on the responsible and trustworthy use and development of AI. My plan is not to write another. So, in this cookbook, I gather some best practices from the literature and sectoral use cases, and present them in recipes to implement efficiently in real-life scenarios. This repository contains an extensive list of techniques and experiments as an initial effort to achieve this goal in the "digital economy" domain. In the digital economy domain, we selected three use cases:

::::{grid}
:gutter: 3

:::{grid-item-card}
**Fair Use of LLMs in Finance**

Developing more fair and equitable applications (e.g. credit scoring, financial news sentiment analysis, market prediction) powered by AI.

[➡️ See the content](./usecases/finance/introduction.md)
:::

:::{grid-item-card}
**A Fairness Benchmark for Facial Biometrics**

Improving biometric facial verification using synthetic data generated by foundational models (e.g. diffusion models)

[➡️ See the content](./usecases/biometrics/introduction.md)
:::

:::{grid-item-card}
**Equitable Digital Public Infrastructure**

AI has the potential to transform many public services to increase their accessibility by the disadvantaged groups.

[➡️ See the content](./usecases/dpi/introduction.md)

:::
::::

Throughout the book, we also explore and demonstrate using some of the AI safety and fairness Python libraries including:

- [faid](https://github.com/asabuncuoglu13/faid): A library for fairness logging and monitoring developed in the Alan Turing Institute.
- [fairlearn](https://github.com/fairlearn/fairlearn): A toolkit to assess and improve the fairness of machine learning models, supported by Microsoft.
- [aqueitas](https://github.com/dssg/aequitas): An open-source bias and fairness audit toolkit for machine learning, developed by Center for Data Science and Public Policy.
- [inspect_ai](https://github.com/UKGovernmentBEIS/inspect_ai): AI Safety Institute's library for inspecting and understanding frontier AI models.
- [LIT](https://github.com/PAIR-code/lit): Learning Interpretability Tool (LIT) provides a set of interactive interpretability visualisations in one interface.
- [llm_comprator](https://github.com/PAIR-code/llm-comparator): Google PAIR's tool for comparing large language models.

We included hands-on experiments to see the impact of bias evaluation and mitigation approaches in action. In a complex product development ecosystem, it is hard to achive "trustworthiness," as all sub-modules of a product is expected to achieve trustworthiness. Therefore, the desirable way to achieve trustworthiness is following a proactive "by design" approach in the product design. In this cookbook, we explored the ways to achieve this kind of proactive approach, however implementing them in a product lifecycle is still up to tangible and intangible capabilities of development teams.

## Contents

Although implementing "best practices" in equitable AI domain requires a cross disciplinary effort, we focused on the sociotechnical and technical techniques, particularly fairness, security and privacy. We do not go into details on governance, auditing techniques, comprehensive assurance.

```{tableofcontents}
```
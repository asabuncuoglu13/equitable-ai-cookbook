---
---


%% Fairness %%

@InProceedings{zemel_13,
  title = 	 {Learning Fair Representations},
  author = 	 {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {325--333},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/zemel13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/zemel13.html},
  abstract = 	 {We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a  whole), and individual fairness (similar individuals should be treated similarly).  We formulate fairness as an optimization problem of finding a  good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group.  We show positive results of our algorithm relative to other known techniques, on three datasets.  Moreover, we demonstrate several advantages to our approach.  First, our intermediate representation can be used for other classification tasks (i.e., transfer  learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.}
}

@inproceedings{hardt_equality_2016,
	title = {Equality of {Opportunity} in {Supervised} {Learning}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati},
	editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
	year = {2016},
}


@misc{kleinberg_inherent_2016,
	title = {Inherent {Trade}-{Offs} in the {Fair} {Determination} of {Risk} {Scores}},
	url = {http://arxiv.org/abs/1609.05807},
	abstract = {Recent discussion in the public sphere about algorithmic classiﬁcation has involved tension between competing notions of what it means for a probabilistic classiﬁcation to be fair to different groups. We formalize three fairness conditions that lie at the heart of these debates, and we prove that except in highly constrained special cases, there is no method that can satisfy these three conditions simultaneously. Moreover, even satisfying all three conditions approximately requires that the data lie in an approximate version of one of the constrained special cases identiﬁed by our theorem. These results suggest some of the ways in which key notions of fairness are incompatible with each other, and hence provide a framework for thinking about the trade-offs between them.},
	language = {en},
	urldate = {2024-05-03},
	publisher = {arXiv},
	author = {Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
	month = nov,
	year = {2016},
	note = {arXiv:1609.05807 [cs, stat]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: To appear in Proceedings of Innovations in Theoretical Computer Science (ITCS), 2017},
	file = {Kleinberg et al. - 2016 - Inherent Trade-Offs in the Fair Determination of R.pdf:/Users/asabuncuoglu/Zotero/storage/KTUH6WP4/Kleinberg et al. - 2016 - Inherent Trade-Offs in the Fair Determination of R.pdf:application/pdf},
}


@inproceedings{galhotra_fairness_2017,
	address = {Paderborn Germany},
	title = {Fairness testing: testing software for discrimination},
	isbn = {978-1-4503-5105-8},
	shorttitle = {Fairness testing},
	url = {https://dl.acm.org/doi/10.1145/3106237.3106277},
	doi = {10.1145/3106237.3106277},
	language = {en},
	urldate = {2024-05-03},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Galhotra, Sainyam and Brun, Yuriy and Meliou, Alexandra},
	month = aug,
	year = {2017},
	pages = {498--510},
	file = {Full Text PDF:/Users/asabuncuoglu/Zotero/storage/WWSA99AH/Galhotra et al. - 2017 - Fairness testing testing software for discriminat.pdf:application/pdf},
}

@inproceedings{dwork_fairness_2012,
	address = {New York, NY, USA},
	series = {{ITCS} '12},
	title = {Fairness through awareness},
	isbn = {978-1-4503-1115-1},
	url = {https://dl.acm.org/doi/10.1145/2090236.2090255},
	doi = {10.1145/2090236.2090255},
	abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
	urldate = {2024-05-03},
	booktitle = {Proceedings of the 3rd {Innovations} in {Theoretical} {Computer} {Science} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
	month = jan,
	year = {2012},
	pages = {214--226},
	file = {Full Text PDF:/Users/asabuncuoglu/Zotero/storage/AGUNAIC7/Dwork et al. - 2012 - Fairness through awareness.pdf:application/pdf},
}


%%== Bias - Media and News Analysis ==%%

@inproceedings{alam_towards_2022,
	address = {Virtual Event, Lyon France},
	title = {Towards {Analyzing} the {Bias} of {News} {Recommender} {Systems} {Using} {Sentiment} and {Stance} {Detection}},
	isbn = {978-1-4503-9130-6},
	url = {https://dl.acm.org/doi/10.1145/3487553.3524674},
	doi = {10.1145/3487553.3524674},
	language = {en},
	urldate = {2024-05-10},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2022},
	publisher = {ACM},
	author = {Alam, Mehwish and Iana, Andreea and Grote, Alexander and Ludwig, Katharina and Müller, Philipp and Paulheim, Heiko},
	month = apr,
	year = {2022},
	pages = {448--457},
	file = {Full Text PDF:/Users/asabuncuoglu/Zotero/storage/TWSJ4JCP/Alam et al. - 2022 - Towards Analyzing the Bias of News Recommender Sys.pdf:application/pdf},
}


@techreport{cdei_2020,
	title = {Review into bias in algorithmic decision-making},
	url = {https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/957259/Review_into_bias_in_algorithmic_decision-making.pdf?_ga=2.220650200.15383270.1715354345-1123678490.1697577586},
	language = {en},
	author = {{Centre for Data Ethics and Innovation}},
	institution = {{Centre for Data Ethics and Innovation}},
	year = {2020},
	file = {Review into bias in algorithmic decision-making.pdf:/Users/asabuncuoglu/Zotero/storage/E77S4INJ/Review into bias in algorithmic decision-making.pdf:application/pdf},
}

%% Data Evaluation %%

@inproceedings{spinde_neural_2021,
	address = {Punta Cana, Dominican Republic},
	title = {Neural {Media} {Bias} {Detection} {Using} {Distant} {Supervision} {With} {BABE} - {Bias} {Annotations} {By} {Experts}},
	url = {https://aclanthology.org/2021.findings-emnlp.101},
	doi = {10.18653/v1/2021.findings-emnlp.101},
	abstract = {Media coverage has a substantial effect on the public perception of events. Nevertheless, media outlets are often biased. One way to bias news articles is by altering the word choice. The automatic identification of bias by word choice is challenging, primarily due to the lack of a gold standard data set and high context dependencies. This paper presents BABE, a robust and diverse data set created by trained experts, for media bias research. We also analyze why expert labeling is essential within this domain. Our data set offers better annotation quality and higher inter-annotator agreement than existing work. It consists of 3,700 sentences balanced among topics and outlets, containing media bias labels on the word and sentence level. Based on our data, we also introduce a way to detect bias-inducing sentences in news articles automatically. Our best performing BERT-based model is pre-trained on a larger corpus consisting of distant labels. Fine-tuning and evaluating the model on our proposed supervised data set, we achieve a macro F1-score of 0.804, outperforming existing methods.},
	urldate = {2024-04-15},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Spinde, Timo and Plank, Manuel and Krieger, Jan-David and Ruas, Terry and Gipp, Bela and Aizawa, Akiko},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {1166--1177},
}

@article{raza_dbias_2024,
	title = {Dbias: detecting biases and ensuring fairness in news articles},
	volume = {17},
	issn = {2364-4168},
	shorttitle = {Dbias},
	url = {https://doi.org/10.1007/s41060-022-00359-4},
	doi = {10.1007/s41060-022-00359-4},
	abstract = {Because of the increasing use of data-centric systems and algorithms in machine learning, the topic of fairness is receiving a lot of attention in the academic and broader literature. This paper introduces Dbias (https://pypi.org/project/Dbias/), an open-source Python package for ensuring fairness in news articles. Dbias can take any text to determine if it is biased. Then, it detects biased words in the text, masks them, and suggests a set of sentences with new words that are bias-free or at least less biased. We conduct extensive experiments to assess the performance of Dbias. To see how well our approach works, we compare it to the existing fairness models. We also test the individual components of Dbias to see how effective they are. The experimental results show that Dbias outperforms all the baselines in terms of accuracy and fairness. We make this package (Dbias) as publicly available for the developers and practitioners to mitigate biases in textual data (such as news articles), as well as to encourage extension of this work.},
	language = {en},
	number = {1},
	urldate = {2024-04-15},
	journal = {International Journal of Data Science and Analytics},
	author = {Raza, Shaina and Reji, Deepak John and Ding, Chen},
	month = jan,
	year = {2024},
	keywords = {Bias, Classification, Deep learning, Entity recognition, Fairness, Masking, Transformer-based models},
	pages = {39--59},
}


@misc{ramshaw_text_1995,
	title = {Text {Chunking} using {Transformation}-{Based} {Learning}},
	url = {http://arxiv.org/abs/cmp-lg/9505040},
	doi = {10.48550/arXiv.cmp-lg/9505040},
	abstract = {Eric Brill introduced transformation-based learning and showed that it can do part-of-speech tagging with fairly high accuracy. The same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive ``baseNP'' chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 92\% for baseNP chunks and 88\% for somewhat more complex chunks that partition the sentence. Some interesting adaptations to the transformation-based learning approach are also suggested by this application.},
	urldate = {2024-04-15},
	publisher = {arXiv},
	author = {Ramshaw, Lance A. and Marcus, Mitchell P.},
	month = may,
	year = {1995},
	note = {arXiv:cmp-lg/9505040},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 13 pages, LaTeX2e, 1 included figure},
}


@article{pipino_data_2002,
	title = {Data quality assessment},
	volume = {45},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/505248.506010},
	doi = {10.1145/505248.506010},
	abstract = {How good is a company's data quality? Answering this question requires usable data quality metrics. Currently, most data quality measures are developed on an ad hoc basis to solve specific problems [6, 8], and fundamental principles necessary for developing usable metrics in practice are lacking. In this article, we describe principles that can help organizations develop usable data quality metrics.},
	number = {4},
	urldate = {2024-04-16},
	journal = {Communications of the ACM},
	author = {Pipino, Leo L. and Lee, Yang W. and Wang, Richard Y.},
	month = apr,
	year = {2002},
	pages = {211--218},
}


@article{wilkinson_fair_2016,
	title = {The {FAIR} {Guiding} {Principles} for scientific data management and stewardship},
	volume = {3},
	copyright = {2016 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata201618},
	doi = {10.1038/sdata.2016.18},
	abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
	language = {en},
	number = {1},
	urldate = {2024-04-16},
	journal = {Scientific Data},
	author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J. G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and ’t Hoen, Peter A. C. and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
	month = mar,
	year = {2016},
	note = {Publisher: Nature Publishing Group},
	keywords = {Publication characteristics, Research data},
	pages = {160018},
}

%% Synthetic Data %%


@misc{nayak_learning_2024,
	title = {Learning to {Generate} {Instruction} {Tuning} {Datasets} for {Zero}-{Shot} {Task} {Adaptation}},
	url = {http://arxiv.org/abs/2402.18334},
	abstract = {We introduce Bonito, an open-source model for conditional task generation: the task of converting unannotated text into task-specific training datasets for instruction tuning. Our goal is to enable zero-shot task adaptation of large language models on users' specialized, private data. We train Bonito on a new large-scale dataset with 1.65M examples created by remixing existing instruction tuning datasets into meta-templates. The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the instruction and the response. We use Bonito to generate synthetic tasks for seven datasets from specialized domains across three task types -- yes-no question answering, extractive question answering, and natural language inference -- and adapt language models. We show that Bonito significantly improves the average performance of pretrained and instruction tuned models over the de facto self supervised baseline. For example, adapting Mistral-Instruct-v2 and instruction tuned variants of Mistral and Llama2 with Bonito improves the strong zero-shot performance by 22.1 F1 points whereas the next word prediction objective undoes some of the benefits of instruction tuning and reduces the average performance by 0.8 F1 points. We conduct additional experiments with Bonito to understand the effects of the domain, the size of the training set, and the choice of alternative synthetic task generators. Overall, we show that learning with synthetic instruction tuning datasets is an effective way to adapt language models to new domains. The model, dataset, and code are available at https://github.com/BatsResearch/bonito.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Nayak, Nihal V. and Nan, Yiyang and Trost, Avi and Bach, Stephen H.},
	month = feb,
	year = {2024},
	note = {arXiv:2402.18334 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

%% Red Teaming %%

@techreport{friedler_ai_2023,
	title = {{AI} {Red}-{Teaming} {Is} {Not} a {One}-{Stop} {Solution} to {AI} {Harms}:},
	language = {en},
	institution = {Data & Society},
	author = {Friedler, Sorelle and Singh, Ranjit and Blili-Hamelin, Borhane and Metcalf, Jacob and Chen, Brian J},
	year = {2023},
}

%% Auditing %%

@article{koshiyama_towards_2021,
	title = {Towards {Algorithm} {Auditing}: {A} {Survey} on {Managing} {Legal}, {Ethical} and {Technological} {Risks} of {AI}, {ML} and {Associated} {Algorithms}},
	issn = {1556-5068},
	shorttitle = {Towards {Algorithm} {Auditing}},
	url = {https://www.ssrn.com/abstract=3778998},
	doi = {10.2139/ssrn.3778998},
	abstract = {Business reliance on algorithms are becoming ubiquitous, and companies are increasingly concerned about their algorithms causing major financial or reputational damage. High-profile cases include VW’s Dieselgate scandal with fines worth of \$34.69B, Knight Capital’s bankruptcy ({\textasciitilde}\$450M) by a glitch in its algorithmic trading system, and Amazon’s AI recruiting tool being scrapped after showing bias against women. In response, governments are legislating and imposing bans, regulators fining companies, and the Judiciary discussing potentially making algorithms artificial “persons” in Law.},
	language = {en},
	urldate = {2024-05-14},
	journal = {SSRN Electronic Journal},
	author = {Koshiyama, Adriano and Kazim, Emre and Treleaven, Philip and Rai, Pete and Szpruch, Lukasz and Pavey, Giles and Ahamat, Ghazi and Leutner, Franziska and Goebel, Randy and Knight, Andrew and Adams, Janet and Hitrova, Christina and Barnett, Jeremy and Nachev, Parashkev and Barber, David and Chamorro-Premuzic, Tomas and Klemmer, Konstantin and Gregorovic, Miro and Khan, Shakeel and Lomas, Elizabeth},
	year = {2021},
	file = {Koshiyama et al. - 2021 - Towards Algorithm Auditing A Survey on Managing L.pdf:/Users/asabuncuoglu/Zotero/storage/KJKYAVTD/Koshiyama et al. - 2021 - Towards Algorithm Auditing A Survey on Managing L.pdf:application/pdf},
}

%% Financial Use Cases %%

@article{wu_bloomberggpt_2023,
	title = {Bloomberggpt: {A} large language model for finance},
	journal = {arXiv preprint arXiv:2303.17564},
	author = {Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
	year = {2023},
	note = {Type: Journal Article},
}


@article{das_fairness_2021,
	title = {Fairness {Measures} for {Machine} {Learning} in {Finance}},
	volume = {3},
	issn = {2640-3943},
	url = {http://pm-research.com/lookup/doi/10.3905/jfds.2021.1.075},
	doi = {10.3905/jfds.2021.1.075},
	abstract = {We present a machine learning pipeline for fairness-aware machine learning (FAML) in ﬁnance that encompasses metrics for fairness (and accuracy). Whereas accuracy metrics are well understood and the principal ones used frequently, there is no consensus as to which of several available measures for fairness should be used in a generic manner in the ﬁnancial services industry. We explore these measures and discuss which ones to focus on, at various stages in the ML pipeline, pre-training and post-training, and we also examine simple bias mitigation approaches. Using a standard dataset we show that the sequencing in our FAML pipeline offers a cogent approach to arriving at a fair and accurate ML model. We discuss the intersection of bias metrics with legal considerations in the US, and the entanglement of explainability and fairness is exempliﬁed in the case study. We discuss possible approaches for training ML models while satisfying constraints imposed from various fairness metrics, and the role of causality in assessing fairness.},
	language = {en},
	number = {4},
	urldate = {2024-05-16},
	journal = {The Journal of Financial Data Science},
	author = {Das, Sanjiv and Donini, Michele and Gelman, Jason and Haas, Kevin and Hardt, Mila and Katzman, Jared and Kenthapadi, Krishnaram and Larroy, Pedro and Yilmaz, Pinar and Zafar, Muhammad Bilal},
	month = oct,
	year = {2021},
	pages = {33--64},
	file = {Das et al. - 2021 - Fairness Measures for Machine Learning in Finance.pdf:/Users/asabuncuoglu/Zotero/storage/L4QTBYH2/Das et al. - 2021 - Fairness Measures for Machine Learning in Finance.pdf:application/pdf},
}


@inproceedings{sawhney_deep_2020,
	address = {Online},
	title = {Deep {Attentive} {Learning} for {Stock} {Movement} {Prediction} {From} {Social} {Media} {Text} and {Company} {Correlations}},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.676},
	doi = {10.18653/v1/2020.emnlp-main.676},
	abstract = {In the ﬁnancial domain, risk modeling and proﬁt generation heavily rely on the sophisticated and intricate stock movement prediction task. Stock forecasting is complex, given the stochastic dynamics and non-stationary behavior of the market. Stock movements are inﬂuenced by varied factors beyond the conventionally studied historical prices, such as social media and correlations among stocks. The rising ubiquity of online content and knowledge mandates an exploration of models that factor in such multimodal signals for accurate stock forecasting. We introduce an architecture that achieves a potent blend of chaotic temporal signals from ﬁnancial data, social media, and inter-stock relationships via a graph neural network in a hierarchical temporal fashion. Through experiments on real-world S\&P 500 index data and English tweets, we show the practical applicability of our model as a tool for investment decision making and trading.},
	language = {en},
	urldate = {2024-05-15},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Sawhney, Ramit and Agarwal, Shivam and Wadhwa, Arnav and Shah, Rajiv Ratn},
	year = {2020},
	pages = {8415--8426},
	file = {Sawhney et al. - 2020 - Deep Attentive Learning for Stock Movement Predict.pdf:/Users/asabuncuoglu/Zotero/storage/MW3YMWZ3/Sawhney et al. - 2020 - Deep Attentive Learning for Stock Movement Predict.pdf:application/pdf},
}

%% UK Reports %%


@techreport{cdei_24,
	title = {Public attitudes to data and {AI}: {Tracker} survey ({Wave} 3) ({Section} 6: {Attitudes} towards {AI})},
	shorttitle = {Public attitudes to data and {AI}},
	url = {https://www.gov.uk/government/publications/public-attitudes-to-data-and-ai-tracker-survey-wave-3/public-attitudes-to-data-and-ai-tracker-survey-wave-3},
	language = {en},
	date = {2024},
	year = {2024},
	author = {{Centre for Data Ethics and Innovation} and {Department for Science, Innovation \& Technology}},
	institution = {{Centre for Data Ethics and Innovation}},
	file = {Snapshot:/Users/asabuncuoglu/Zotero/storage/T9Y2RFLE/public-attitudes-to-data-and-ai-tracker-survey-wave-3.html:text/html},
}


%% Global Reports %%


@techreport{butler_microsoft_2023,
	title = {Microsoft {New} {Future} of {Work} {Report} 2023},
	url = {https://www.microsoft.com/en-us/research/publication/microsoft-new-future-of-work-report-2023/},
	abstract = {In the past three years, there have been not one but two generational shifts in how work gets done, both of which were only possible because of decades of research and development. The first shift occurred when COVID made us realize how powerful remote and hybrid work technologies had become, as well as how much science was available to guide us in how to (and how not to) use these technologies. The second arrived this year, as it became clear that, at long last, generative AI had advanced to the point where it could be valuable to huge swaths of the work people do every day. We began the New Future of Work Report series in 2021, at the height of the shift to remote work. The goal of that report was to provide a synthesis of new – and newly relevant – research to anyone interested in reimagining work for the better as a decades-old approach to work was challenged. The second New Future of Work Report, published in 2022, focused on hybrid work and what research could teach us about intentionally re-introducing co-location into people’s work practices. This year’s edition, the third in the series, continues with the same goal, but centers on research related to integrating LLMs into work. Throughout 2023, AI and the future of work have frequently been on the metaphorical – and often literal – front page around the world. There have been many excellent articles about the ways in which work may change as LLMs are increasingly integrated into our lives. As such, in this year’s report we focus specifically on areas that we think deserve additional attention or where there is research that has been done at Microsoft that offers a unique perspective. This is a report that should be read as a complement to the existing literature, rather than as a synthesis of all of it. This is a rare time, one in which research will play a particularly important role in defining what the future of work looks like. At this special moment, scientists can’t just be passive observers of what is happening. Rather, we have the responsibility to shape work for the better. We hope this report can help our colleagues around world make progress towards this goal.},
	number = {MSR-TR-2023-34},
	institution = {Microsoft},
	author = {Butler, Jenna and Jaffe, Sonia and Baym, Nancy and Czerwinski, Mary and Iqbal, Shamsi and Nowak, Kate and Rintel, Sean and Sellen, Abigail and Vorvoreanu, Mihaela and Abdulhamid, Najeeb G. and Amores, Judith and Andersen, Reid and Awori, Kagonya and Axmed, Maxamed and boyd, danah and Brand, James and Buscher, Georg and Carignan, Dean and Chan, Martin and Coleman, Adam and Counts, Scott and Daepp, Madeleine and Fourney, Adam and Goldstein, Daniel G. and Gordon, Andy and Halfaker, Aaron L and Hernandez, Javier and Hofman, Jake and Lay-Flurrie, Jenny and Liao, Vera and Lindley, Siân and Manivannan, Sathish and Mcilwain, Charlton and Nepal, Subigya and Neville, Jennifer and Nyairo, Stephanie and O'Neill, Jacki and Poznanski, Victor and Ramos, Gonzalo and Rangan, Nagu and Rosedale, Lacey and Rothschild, David and Safavi, Tara and Sarkar, Advait and Scott, Ava and Shah, Chirag and Shah, Neha Parikh and Shapiro, Teny and Shaw, Ryland and Simkute, Auste and Suh, Jina and Suri, Siddharth and Tanase, Ioana and Tankelevitch, Lev and Troy, Adam and Wan, Mengting and White, Ryen W. and Yang, Longqi and Hecht, Brent and Teevan, Jaime},
	month = dec,
	year = {2023},
}


@techreport{cabinet_24,
	title = {Ethics, {Transparency} and {Accountability} {Framework} for {Automated} {Decision}-{Making}},
	url = {https://www.gov.uk/government/publications/ethics-transparency-and-accountability-framework-for-automated-decision-making/ethics-transparency-and-accountability-framework-for-automated-decision-making},
	language = {en},
	year = {2024},
	urldate = {2024-03-18},
	institution = {Centre for Data Ethics and Innovation},
	author = {{Cabinet Office} and {Centre for Data Ethics and Innovation} and {Department for Science, Innovation \& Technology} and {Office for Artificial Intelligence}},
	file = {Snapshot:/Users/asabuncuoglu/Zotero/storage/XBJDPH5Q/ethics-transparency-and-accountability-framework-for-automated-decision-making.html:text/html},
}


@article{wachter_counterfactual_2017,
	title = {Counterfactual {Explanations} {Without} {Opening} the {Black} {Box}: {Automated} {Decisions} and the {GDPR}},
	issn = {1556-5068},
	shorttitle = {Counterfactual {Explanations} {Without} {Opening} the {Black} {Box}},
	url = {https://www.ssrn.com/abstract=3063289},
	doi = {10.2139/ssrn.3063289},
	language = {en},
	urldate = {2024-05-16},
	journal = {SSRN Electronic Journal},
	author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
	year = {2017},
	file = {Wachter et al. - 2017 - Counterfactual Explanations Without Opening the Bl.pdf:/Users/asabuncuoglu/Zotero/storage/C8WFSWGV/Wachter et al. - 2017 - Counterfactual Explanations Without Opening the Bl.pdf:application/pdf},
}


@techreport{boe_machine_2022,
	title = {Machine learning in {UK} financial services},
	url = {https://www.bankofengland.co.uk/report/2022/machine-learning-in-uk-financial-services},
	abstract = {The Bank of England and Financial Conduct Authority conducted a second survey into the state of machine learning in UK financial services.},
	language = {en},
	urldate = {2024-05-20},
	author = {{The Bank of England}},
	institution = {{The Bank of England}},
	year = {2022},
	file = {Snapshot:/Users/asabuncuoglu/Zotero/storage/YY83SJCF/machine-learning-in-uk-financial-services.html:text/html},
}


@techreport{boe_financial_2023,
	title = {Financial {Policy} {Summary} and {Record} of the {Financial} {Policy} {Committee} meeting on 21 {November}},
	url = {https://www.bankofengland.co.uk/-/media/boe/files/financial-policy-summary-and-record/2023/fpc-summary-and-record-december-2023.pdf},
	urldate = {2024-05-20},
	author = {{The Bank of England}},
	institution = {{The Bank of England}},
	year = {2023},
	file = {fpc-summary-and-record-december-2023.pdf:/Users/asabuncuoglu/Zotero/storage/ZTY5ZLIJ/fpc-summary-and-record-december-2023.pdf:application/pdf},
}

%% News %%


@misc{prnewswire_large_2023,
	title = {Large {Language} {Model} ({LLM}) {Market} {Size} to {Grow} {USD} 40.8 {Billion} by 2029 at a {CAGR} of 21.4\% {\textbar} {Valuates} {Reports}},
	url = {https://finance.yahoo.com/news/large-language-model-llm-market-151500260.html},
	abstract = {The LLM Market is Segmented by Type (Hundreds of Billions of Parameters, Trillions of Parameters), by Application (Medical, Minancial, Industrial, Education): Global Opportunity Analysis and Industry Forecast, 2023-2029. It is Published in Valuates Reports Under the Category of Software.},
	language = {en-US},
	urldate = {2024-05-20},
	journal = {Yahoo Finance},
	author = {{PRNewswire}},
	month = sep,
	year = {2023},
	file = {Snapshot:/Users/asabuncuoglu/Zotero/storage/WQD8VQSR/large-language-model-llm-market-151500260.html:text/html},
}

@misc{j_p_morgan_private_bank_how_2024,
	title = {How to invest in {AI}’s next phase {\textbar} {J}.{P}. {Morgan} {Private} {Bank} {U}.{S}.},
	url = {https://privatebank.jpmorgan.com/nam/en/insights/markets-and-investing/ideas-and-insights/how-to-invest-in-ais-next-phase},
	abstract = {We think it’s time for investors who want to buy into AI to expand their approach. Here’s how you could consider investing.},
	language = {en},
	urldate = {2024-05-20},
	author = {{J. P. Morgan Private Bank}},
	year = {2024},
	file = {Snapshot:/Users/asabuncuoglu/Zotero/storage/5435AJZ8/how-to-invest-in-ais-next-phase.html:text/html},
}

